\documentclass{scrartcl}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

%die Art der Aufzählung lässt sich hiermit verändern
\usepackage{paralist}

%Diese drei Pakete benötigen wir für die Umlaute, Deutsche Silbentrennung etc.
%Apple-Nutzer sollten anstelle von \usepackage[latin1]{inputenc} das Paket \usepackage[applemac]{inputenc} verwenden
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
%für Windows
\usepackage[utf8]{inputenc}

% for German citation style
% \usepackage[options]{natbib}
\usepackage{harvard} 
\renewcommand{\harvardand}{und} 

%entfernt die roten Rechtecke um verlinkte Worte
\hypersetup{
    pdfborder = {0 0 0}
}

%\setkomafont{disposition}{\normalfont\bfseries}
\author{Julian Baumann, Xenia Kühling, Sebastian Ruder}

\date{\today}

\title{Regelbasierte Koreferenz mit BART}

\subtitle{Algorithmen und Implementation zum Softwareprojekt im SS14}

%Brücke zwischen algorithmischer und implementatorischer Ebene
%abstrakter, mit Bezug zu den computerlinguistischen Fragestellungen

\begin{document}
\maketitle
\section{BART im Vergleich zu Stanford}

BART unternimmt automatische Koreferenzresolution mithilfe einer modularen Pipeline, die aus einer Vorverarbeitungsphase (Daten von MMAX2-Annotationsebenen werden aggregiert), der Extraktion der NP-Kandidaten, der Extraktion der NP-Merkmale und der Kandidatenpaare sowie aus einem Resolutionsmodell besteht.\\
BART verwendet üblicherweise einen auf einem Ansatz von \citeasnoun{soon2001} basierenden Resolutionsalgorithmus, der Kandidaten-NPs hinsichtlich ihrer Merkmale paarweise vergleicht. Statt diesem implementierten wir das Resolutionssystem der Stanford-NLP-Gruppe um \citeasnoun{lee2013}, das sich durch seine Sieb-Architektur auszeichnet und regel-basiert ist.\\
Frühe Systeme zur Koreferenzresolution basierten ebenfalls auf Regeln. Aufgrund ihrer Abhängigkeit von manuell zu bestimmenden Gewichten und ihrer Unfähigkeit, Koreferenten nicht nur paarweise, sondern korpusübergreifend zuzuordnen, werden heute jedoch stattdessen zumeist Machine-Learning-Systeme eingesetzt. Während überwachte Systeme allerdings auf manuell annotierte Daten angewiesen sind, lassen sich unüberwachte Systeme hingegen aufgrund ihrer Komplexität nur schwierig auf neue Domänen übertragen.\\
\citeasnoun{lee2013} suchen, die Vorteile Regel- und Machine-Learning-basierter Systeme zu verheiraten und konnten so mit ihrem regel-basierten System das beste Ergebnis beim CoNLL-2011 \textit{shared task} erzielen. Im Rahmen der Sieb-Architektur werden nacheinander - absteigend nach ihrer Präzision geordnet - eine Reihe von deterministischen Koreferenzmodellen angewendet, wobei jedes Modell auf den Output seines Vorgängers aufbaut. Besonders das Entität-zentrische Modell, bei dem Merkmale über alle Vorkommen einer Entität geteilt werden, ermöglicht die globale Koreferenzresolution für regel-basierte Systeme.

\section{Allgemeine Implementation}

Die Klasse \texttt{SieveAnnotator} ruft für jedes Dokument die \texttt{decodeDocument}-Methode des \texttt{SieveDecoder} auf, der \texttt{corefResolver} als Interface implementiert. In dieser wird über alle \textit{mentions} dieses Dokumentes zehnmal iteriert, wobei bei jeder Iteration durch eine \texttt{SieveFactory} ein anderer \textit{sieve} aufgerufen wird. Hier filtern wir bereits alle indefiniten \textit{mentions} heraus, da diese sich meist auf generische Konzepte beziehen, die keinen Koreferenten besitzen.\\
Jeder \textit{sieve} erbt von der abstrakten \texttt{Sieve}-Klasse, die \texttt{runSieve}-Methode, die er für jede \textit{mention} aufruft und in der er unter den Antezedenzien nach einem Koreferenten für diese \textit{mention} sucht. \texttt{Sieve} enthält bereits alle \textit{utility}-Methoden, die von den Subklassen verwendet werden und von denen die meisten als Input eine \texttt{PairInstance} bestehend aus \textit{mention} und Antezedens annehmen. Zur Bestimmung mancher Merkmale verwenden wir darüber hinaus die in BART bereits implementierten \texttt{PairFeatureExtractors}.\\
Die Merkmale koreferenter \textit{mentions} werden in einer eigenen \texttt{DiscourseEntity}-Klasse geteilt. Weitere sprachspezifische Informationen ergänzten wir in den entsprechenden \texttt{LinguisticConstants} sowie durch Methoden zum Nachschlagen in zusammengestellten Listen im korrespondierenden \texttt{LanguagePlugin}.

\section{Details zu den einzelnen Sieves}

\begin{itemize}

\item \textbf{SpeakerIdentification} identifiziert den Sprecher und verbindet ihn mit möglichen koreferenten Pronomen in wörtlicher Rede. Da der von \citeasnoun{lee2013} verwendete OntoNotes-Korpus auch Telefon-Gespräche und Talk Shows umfasst, erzielt dieser Sieve bereits einen Recall von 8,7\% und einen F1-Score von 15,5\% (MUC). Da der TüBa-D/Z-Korpus hingegen nur aus Nachrichtenartikeln besteht, ist der Effekt dieses \textit{sieve} signifikant geringer. Handelt es sich bei einem der beiden \textit{mentions} um ein Vorfeld-Es, so wird keine Übereinstimmung gefunden. Numerus-Äquivalenz und eine Satzentfernung ${\leq}$ 1 müssen gegeben sein. Es wird mit BARTs \texttt{FE\_Speech} überprüft, ob sich eine der \textit{mentions} in wörtlicher Rede befindet und auf Grundlage der Position von ':' und '"' sowie Sprechverben ausgemacht, wo sich der Sprecher befindet, woraufhin schließlich sichergestellt wird, dass die sich in der wörtlichen Rede befindende \textit{mention} weder Reflexiv-, noch Relativ-, sondern lediglich Pronomen ist.

\item \textbf{StringMatch} sieht zwei \textit{mentions} als koreferent an, wenn sie exakt übereinstimmen (einschließlich Modifikatoren und Artikel). Wir überprüfen dies, indem wir den String der \textit{markables} der beiden \textit{mentions} vergleichen. Zudem schließen wir indefinite \textit{mentions} aus sowie \textit{mentions}, deren String eine Datumsangabe enthält, da gleiche Daten in TüBa-D/Z nicht als koreferent gelten. \citeasnoun{lee2013} geben hier eine Präzision von über 90\% $B^3$ an, während wir für das Deutsche und den TüBa-D/Z-Korpus eine etwas geringere Genauigkeit von 86\% erreichen.

\item \textbf{RelaxedStringMatch} gibt zwei \textit{mentions} als koreferent an, sofern sie identisch sind, wenn ihre Postmodifikatoren ignoriert werden.

\item \textbf{PreciseConstructs} matcht zwei \textit{mentions} als koreferent, wenn sie eine der folgenden Bedingungen erfüllen: 
\begin{itemize}
	\item Sie stehen gemeinsam in einer Appositions- oder Subjekt-Objekt-Konstruktion. Diese Bedingung ist für uns unerheblich, da diese Konstruktionen in TüBa-D/Z nicht annotiert werden.
	\item Die \textit{mention} modifiziert den Antezedens-Kopf. Hier weisen wir explizit den deutschen Relativpronomen ein Genus zu und überprüfen dessen Übereinstimmung zwischen \textit{mention} und Antezedens sowie Wortentfernung, wobei wir sicherstellen, dass das Antezedens nicht von einer weiteren \textit{mention} eingebettet wird, auf die sich das Relativpronomen eher beziehen könnte.
	\item Eine der \textit{mentions} ist ein Akronym der anderen. Hier berücksichtigen wir Eigenheiten des Deutschen, in dem auch Akronyme teilweise mit '-' getrennt werden.
	\item Eine der \textit{mentions} ist ein Demonym der anderen. Hierfür schlagen wir Demonyme in einer auf Wikipedia basierenden Liste nach, wobei dieses Kriterium im TüBa-D/Z-Korpus nicht von großer Bedeutung ist.
\end{itemize}
\citeasnoun{lee2013} geben für diesen \textit{sieve} eine $B^3$-Präzision von über 90\%, während wir eine Genauigkeit von 81\% erhalten. Da das Relativpronomen-Kriterium in diesem \textit{sieve} überwiegt, liegt diese Differenz vermutlich in der hypotaktischen Struktur des Deutschen (im Vergleich zum Englischen), sowie in der domänspezifischen Häufigkeit extraponierter (und daher schwer zuordenbarer) Relativsätze begründet.

\item \textbf{StrictHeadMatchA} matcht zwei \textit{mentions}, wenn das \textit{head lemma} der Anapher in den \textit{heads} der Antezedens-Entität vorhanden ist und die folgenden Bedingungen erfüllt sind:

\begin{compactenum}[(i)]
 \item Die Modifikatoren der \textit{mention} müssen unter den Modifikatoren des Antezedens sein.
 \item Alle Nicht-Stoppwörter der aktuellen Entität müssen in der Anzedens-Entität vorkommen.
 \item Keine der beiden NPs kann Kind des Konstituenten der anderen NP sein (\textit{i-within-i}).
\end{compactenum}

\item \textbf{StrictHeadMatchB} ignoriert Bedingung (i), während \textbf{StrictHeadMatchC}  auf Bedingung (ii) verzichtet.

\item \textbf{ProperHeadNounMatch} weist zwei \textit{mentions} als koreferent aus, wenn sie dasselbe \textit{head word} besitzen und dieses \textit{head word} ein \textit{proper noun} ist, sowie Bedingung (iii) erfüllen und (iv) keine unterschiedlichen Orte, Namen, Zahlen oder räumliche Modifikatoren aufweisen.

\item \textbf{RelaxedHeadMatch} matcht zwei \textit{mentions}, wenn der Kopf der \textit{mention} mit einem Wort in der Antezedens-Entität übereinstimmt, wobei beide \textit{named entities} desselben Typs sein müssen und Bedingungen (ii) und (iii) genügen müssen.

\item \textbf{PronounMatch} matcht Pronomen mit ihrem Antezedens, sofern diese morphologisch kompatibel sind, den Binding-Constraints genügen und nicht mehr als drei Sätze auseinanderliegen. Kataphern werden nicht berücksichtigt. \\
Das Pronomen ist mit demjenigen Antezedens koreferent, das die höchste \textit{salience} hat \cite{wunsch2006}. Die \textit{salience} berechnen wir folgendermaßen:

\begin{itemize}[]
	\item +20 wenn sich  Antezedens und Anapher im selben Satz befinden
	\item +35 wenn Antezedens und Anapher die selbe grammatische Funktion verwenden
	\item +170 wenn der Antezedens ein Subjekt ist
	\item +70 wenn der Antezedens ein Akkusativobjekt ist
	\item +50 wenn der Antezedens ein Dativ oder Genitivobjekt ist
	\item +100 wenn der Antezedens ein proper noun ist
\end{itemize}

Die endgültige \textit{salience} ist abhängig von der Satzentfernung $d$: $S = \frac{s}{2^d}$. \\
Wir berechnen die \textit{salience} wie \citeasnoun{wunsch2006}, berücksichtigen jedoch keine Kataphern und keine Antezedenten, die mehr als 3 Sätze entfernt sind. Außerdem verwenden wir keine die \textit{head noun emphasis}, führen aber zusätzlich \textit{proper noun} als Feature ein.  
\end{itemize}

\section{Vergleich der Datenformate und Korpora}
Das XML-Format der TüBa-D/Z wird für BART in MMAX2 \cite{muller2006} konvertiert, das die Daten in verschiedenen \textit{markable levels} bereitstellt, wobei auf benötigte Diskurselemente, wie z.B. grammatische Funktionen mit \texttt{getDiscourseElementsByLevel} zugegriffen werden kann. Wie bereits eingangs erwähnt unterscheiden sich die Annotationsrichtlinien der verwendeten Korpora und ihre Datenformate hinsichtlich der Konstruktionen, die annotiert werden und ihrer Domäne.

\section{Evaluation}
Aufgrund der Schwierigkeit, Daten im MMAX2-Format zu analysieren und zu visualisieren, konvertieren wir die Texte in das HTML-Format, wobei wir koreferente \textit{mentions} farbig markieren. Mithilfe der \texttt{Evaluation}-Klasse lassen wir uns ebenfalls die Details jedes \textit{match} und die Sieb-spezifische Performanz ausgeben.\\
Wir evaluierten gegen BART für das Deutsche und gegen das Stanford-System für das Englische. Als Testkorpus für das Deutsche verwendeten wir die ersten 99 Dokumente der TüBa-D/Z, wobei wir BARTs \textit{machine learning}-Komponente auf den restlichen Dokumenten der TüBa-D/Z (Nr. 100 - Nr. 3528) trainierten. In Tabelle \ref{tab:ml_vergleich} ist der Vergleich mit BART zu sehen, während aus Tabelle \ref{tab:links} die Anzahl der verlinkten \textit{mentions} jedes \textit{sieve}  aus Tabelle \ref{tab:sieve_performanz} der individuelle Performanzgewinn hervorgehen.
\begin{table}[h]
\begin{tabular}{l||ll|l}
& \multicolumn{3}{c}{\textbf{MUC-Score}} \\ \hline
               & \textbf{Recall}		 & \textbf{Precision} & \textbf{F\_1}    \\ \hline
Unser System 	& 0.644      & 0.691              & 0.667  \\
BART  & 0.721 		 & 0.532     & 0.612
\end{tabular}
\caption{Vergleich mit BARTs Machine Learning-Konfiguration (\texttt{XMLExperiment})}
\label{tab:ml_vergleich}
\end{table}

\begin{table}[h]
\begin{tabular}{l||l|l|l}
\textbf{Sieve} & \textbf{\# Links} & \textbf{\# korrekte Links} & \textbf{Präzision} \\\hline
SpeakerIdentificationSieve & 11 & 7 & 0.636 \\
StringMatchSieve & 324 & 280 & 0.864\\
RelaxedStringMatchSieve & 67 & 44 & 0.657 \\
PreciseConstructSieve & 139 & 113 & 0.813 \\
StrictHeadMatchASieve & 136 & 104 & 0.765 \\
StrictHeadMatchBSieve & 182 & 118 & 0.648 \\
StrictHeadMatchCSieve & 12 & 6 & 0.500 \\
ProperHeadNounMatchSieve & 2 & 2 & 1.000 \\
RelaxedHeadMatchSieve & 72 & 56 & 0.778 \\
PronounMatchSieve & 797 & 460 & 0.577
\end{tabular}
\caption{Übersicht über die Anzahl der verlinkten \textit{mentions} der \textit{sieves}}
\label{tab:links}
\end{table}

\begin{table}[h]
\begin{tabular}{l||ll|l}
& \multicolumn{3}{c}{\textbf{MUC-Score}} \\ \hline
	                 & \textbf{Recall} & \textbf{Precision} & \textbf{F\_1} \\ \hline
SpeakerIdentification & 0.004 & 0.637 & 0.008 \\
+StringMatch & 0.157 & 0.857 & 0.265 \\
+RelaxedStringMatch & 0.180 & 0.825 & 0.295 \\
+PreciseConstructs & 0.241 & 0.822 & 0.372 \\
+HeadMatchA & 0.295 & 0.809 & 0.432 \\
+HeadMatchB & 0.355 & 0.775 & 0.487 \\
+HeadMatchC & 0.357 & 0.771 & 0.488 \\
+ProperHeadNounMatch & 0.358 & 0.771 & 0.489 \\
+RelaxedHeadMatch & 0.383 & 0.771 & 0.512 \\
+PronounMatch & 0.644 & 0.691 & 0.667 \\ 
\end{tabular}
\caption{Performanz der einzelnen \textit{sieves}}
\label{tab:sieve_performanz}
\end{table}

\clearpage
Für die Evaluation unseres Systems auf englischsprachigen Daten, die aus Tabelle \ref{tab:ml_vergleich} hervorgeht, verwendeten wir das Trainings-Set des CoNLL-2012 \textit{shared task}, das auf dem OntoNotes 5.0-Korpus basiert. Die Daten wurden mithilfe mehrerer Skripte von Olga Uryupina in das MMAX2-Format konvertiert und außerdem von der BART-eigenen \textit{preprocessing}-Pipeline vorverarbeitet.\\
Da wir unser System vorrangig für das Deutsche entwickelten, sind diese Ergebnisse deutlich ausbaufähig. Der PronounMatchSieve konnte zudem nicht verwendet werden, da er -- anders als der PronounMatchSieve des Stanford-Systems -- auf grammatischen Funktionen basiert. Eine Ebene, die diese darstellt, war allerdings nicht verfügbar. \\

\begin{table}[h]
\begin{tabular}{l||ll|l}
& \multicolumn{1}{c}{\textbf{MUC-Score}} \\ \hline
          &    \textbf{F\_1}    \\ \hline
Unser System &	  0.420  \\
Stanford   &	0.603
\end{tabular}
\caption{Vergleich mit dem Stanford-System}
\label{tab:ml_vergleich}
\end{table}


\clearpage

\nocite{*}
\renewcommand*{\refname}{} % This will define heading of bibliography to be empty, so you can...
\section{Literatur}  
\bibliography{lit}{}
\bibliographystyle{agsm}

\end{document}
