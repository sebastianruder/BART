\documentclass{scrartcl}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

%die Art der Aufzählung lässt sich hiermit verändern
\usepackage{paralist}

%Diese drei Pakete benötigen wir für die Umlaute, Deutsche Silbentrennung etc.
%Apple-Nutzer sollten anstelle von \usepackage[latin1]{inputenc} das Paket \usepackage[applemac]{inputenc} verwenden
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
%für Windows
\usepackage[utf8]{inputenc}

% for German citation style
% \usepackage[options]{natbib}
\usepackage{harvard} 
\renewcommand{\harvardand}{und} 

%\setkomafont{disposition}{\normalfont\bfseries}
\author{Julian Baumann, Xenia Kühling, Sebastian Ruder}

\date{08. August 2014}

\title{Regelbasierte Koreferenz mit BART}

\subtitle{Algorithmen und Implementation zum Softwareprojekt im SS14}

%Brücke zwischen algorithmischer und implementatorischer Ebene
%abstrakter, mit Bezug zu den computerlinguistischen Fragestellungen

\begin{document}
\maketitle
\section{BART im Vergleich zu Stanford}

BART unternimmt automatische Koreferenzresolution mithilfe einer modularen Pipeline, die aus einer Vorverarbeitungsphase (Daten von MMAX2-Annotationsebenen werden aggregiert), der Extraktion der NP-Kandidaten, der Extraktion der NP-Merkmale und der Kandidatenpaare sowie aus einem Resolutionsmodell besteht.

BART verwendet momentan einen auf einem Ansatz von \citeasnoun{soon2001} basierenden Resolutionsalgorithmus, der Kandidaten-NPs hinsichtlich ihrer Merkmale paarweise vergleicht. Statt diesem soll nun das Resolutionssystem der Stanford-NLP-Gruppe (im Folgenden Stanford-System) implementiert werden, das sich durch seine Sieb-Architektur auszeichnet. Obwohl es regel-basiert ist, konnte es dennoch das beste Ergebnis beim CoNLL-2011 shared task erzielen. Im Rahmen der Sieb-Architektur werden nacheinander - absteigend nach ihrer Präzision geordnet - eine Reihe von deterministischen Koreferenzmodellen angewendet, wobei jedes Modell auf den Output seines Vorgängers aufbaut. Besonders das Entität-zentrische Modell, in bei dem Merkmale über alle Vorkommen einer Entität geteilt werden, bietet einen deutlichen Wissensgewinn, der von Nutzen für BARTs Performanz sein wird.

Historisch betrachtet basierten frühe Systeme zur Koreferenzresolution vor allem auf Regeln. Aufgrund ihrer Abhängigkeit von manuell zu bestimmenden Gewichten und ihrer Unfähigkeit, Koreferenten nicht nur paarweise, sondern korpusübergreifend zuzuordnen, wurden sie jedoch heute von machine learning-Systemen ersetzt. Während überwachte Systeme auf manuell annotierte Daten angewiesen sind, lassen sich unüberwachte Systeme aufgrund ihrer Komplexität nur schwierig auf neue Domänen übertragen.

\section{Allgemeine Implementation}

Die Klasse \texttt{SieveAnnotator} ruft für jedes Dokument die \texttt{decodeDocument}-Methode des \texttt{SieveDecoder} auf, der \texttt{corefResolver} als Interface implementiert. In dieser wird über alle mentions dieses Dokumentes zehnmal iteriert, wobei bei jeder Iteration durch eine \texttt{SieveFactory} ein anderer Sieve aufgerufen wird.\\
Jeder Sieve erbt von der abstrakten \texttt{Sieve}-Klasse, die \texttt{runSieve}-Methode, die er für jede mention aufruft und in der er unter den Antezedenten nach einem Koreferenten für diese mention sucht.

\section{Details zu den einzelnen Sieves}

\begin{itemize}

\item \textbf{SpeakerIdentification:} Es werden Sprecher identifiziert und mit möglichen koreferenten Pronomen verbunden. Da der von der Stanford-Gruppe verwendete OntoNotes-Korpus auch Telefon-Gespräche und Talk Shows umfasst, erzielt dieser Sieve bereits einen Recall von 8,7\% und einen F1-Score von 15,5\% (MUC-Score); da der TüBa-D/Z-Korpus hingegen nur aus Nachrichtenartikeln besteht, ist der Effekt dieses Sieves signifikant geringer. Da es sich vor allem um indirekte Rede handelt, schlagen wir Verben nach, die synonym mit "sagen" sind. \citeasnoun{broscheit2010} \cite{broscheit2010}

\item \textbf{StringMatch} sieht zwei mentions als koreferent an, wenn sie exakt übereinstimmen (einschließlich Modifikatoren und Artikel). geben hier eine Precision von über 90\% $B^3$ an. Für das Deutsche und den TüBa-D/Z-Korpus erreicht dieser Sieve allerdings nur eine Precision von ??.

\item \textbf{RelaxedStringMatch} gibt zwei mentions als koreferent an, sofern sie identisch sind, wenn ihre Postmodifikatoren ignoriert werden.

\item \textbf{PreciseConstructs} matcht zwei mentions als koreferent, wenn sie gemeinsam in einer Appositions- oder Subjekt-Objekt-Konstruktion stehen oder wenn die mention ein zum Kopf des Antezedens zugehöriges Relativpronomen oder, ein Akronym oder ein Demonym ist. Aufgrund der Annotationsrichtlinien des TüBa-D/Z-Korpus werden Appositions- und Subjekt-Objekt-Konstruktionen dort nicht getaggt. Auch bei uns rangiert dieser Sieve unter den präzisesten, auch wenn er nicht ganz an die $B^3$-Präzision von über 90\% des Stanford-Systems heranreicht.

\item \textbf{StrictHeadMatchA} matcht mentions, die denselben Head besitzen und darüber hinaus die folgenden Bedingungen erfüllen:

\begin{compactenum}[(i)]
 \item Die Modifikatoren der mention müssen unter den Modifikatoren des Antezedenten sein.
 \item Alle Nicht-Stoppwörter der aktuellen Entität müssen in der Anzedens-Entität vorkommen.
 \item Keine der beiden NPs kann Kind des Konstituenten der anderen NP sein.
\end{compactenum}

\item \textbf{StrictHeadMatchB} entfernt Bedingung (i), während \textbf{StrictHeadMatchC} Bedingung (ii) entfernt.

\item \textbf{ProperHeadWordMatch} weist zwei mentions als Koreferenten aus, wenn sie dasselbe \textit{head word} besitzen, sowie Bedingung (iii) erfüllen und keine unterschiedlichen Orte, Namen, Zahlen oder räumliche Modifikatoren aufweisen.

\item \textbf{RelaxedHeadMatch} matcht zwei mentions, wenn der Kopf der mention mit einem Wort in der Antezedent-Entität übereinstimmt, wobei beide \textit{named entities} desselben Typs sein müssen und Bedingungen (ii) und (iii) genügen müssen.

\item \textbf{PronounMatch} matcht ein Pronomen mit einer Entität, wenn folgende Merkmale übereinstimmen: Numerus, Genus, Person, Belebtheit, NER-Label, Satzentfernung zwischen Pronomen und Antezedens ${\leq}$ 3.

\end{itemize}

\section{Vergleich der Datenformate}
Eigenheiten/Besonderheiten MMAX2
Das XML-Format der TüBa-D/Z wird für BART in MMAX2 konvertiert (zitieren: Müller and Strube, 2006 -Multi-level annotation of linguistic data with MMAX2).

Hierbei gehen durch die Verwendung von MiniDiscourse in BART bspw. grammatische Funktionen verloren.

Vergleich mit OntoNotes-Format

\section{Evaluation}
Aufgrund der Schwierigkeit, Daten im MMAX2-Format zu visualisieren, konvertierten wir die Texte aus unserem Test-Korpus in das HTML-Format, wobei wir koreferente mentions markierten.

Separate Evaluationsklasse. 

Evaluationstabellen
1. Tabelle: Vergleich mit BART Machine Learning-Konfiguration XMLExperiment auf TüBa-D/Z

2. Tabelle: Vergleich mit Stanford-System mit CoNLL Scorer auf OntoNotes

3. Tabelle: Verbesserungen der einzelnen Sieves

\clearpage

\nocite{*}
\renewcommand*{\refname}{} % This will define heading of bibliography to be empty, so you can...
\section{Literatur}  
\bibliography{lit}{}
\bibliographystyle{agsm}

\end{document}
