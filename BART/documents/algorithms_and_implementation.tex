\documentclass{scrartcl}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

%die Art der Aufzählung lässt sich hiermit verändern
\usepackage{paralist}

%Diese drei Pakete benötigen wir für die Umlaute, Deutsche Silbentrennung etc.
%Apple-Nutzer sollten anstelle von \usepackage[latin1]{inputenc} das Paket \usepackage[applemac]{inputenc} verwenden
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
%für Windows
\usepackage[utf8]{inputenc}

% for German citation style
% \usepackage[options]{natbib}
\usepackage{harvard} 
\renewcommand{\harvardand}{und} 

%\setkomafont{disposition}{\normalfont\bfseries}
\author{Julian Baumann, Xenia Kühling, Sebastian Ruder}

\date{\today}

\title{Regelbasierte Koreferenz mit BART}

\subtitle{Algorithmen und Implementation zum Softwareprojekt im SS14}

%Brücke zwischen algorithmischer und implementatorischer Ebene
%abstrakter, mit Bezug zu den computerlinguistischen Fragestellungen

\begin{document}
\maketitle
\section{BART im Vergleich zu Stanford}

BART unternimmt automatische Koreferenzresolution mithilfe einer modularen Pipeline, die aus einer Vorverarbeitungsphase (Daten von MMAX2-Annotationsebenen werden aggregiert), der Extraktion der NP-Kandidaten, der Extraktion der NP-Merkmale und der Kandidatenpaare sowie aus einem Resolutionsmodell besteht.\\
BART verwendet üblicherweise einen auf einem Ansatz von \citeasnoun{soon2001} basierenden Resolutionsalgorithmus, der Kandidaten-NPs hinsichtlich ihrer Merkmale paarweise vergleicht. Statt diesem implementierten wir das Resolutionssystem der Stanford-NLP-Gruppe um \citeasnoun{lee2013}, das sich durch seine Sieb-Architektur auszeichnet und regel-basiert ist.\\
Frühe Systeme zur Koreferenzresolution basierten ebenfalls auf Regeln. Aufgrund ihrer Abhängigkeit von manuell zu bestimmenden Gewichten und ihrer Unfähigkeit, Koreferenten nicht nur paarweise, sondern korpusübergreifend zuzuordnen, werden heute jedoch stattdessen zumeist Machine-Learning-Systeme eingesetzt. Während überwachte Systeme allerdings auf manuell annotierte Daten angewiesen sind, lassen sich unüberwachte Systeme hingegen aufgrund ihrer Komplexität nur schwierig auf neue Domänen übertragen.\\
\citeasnoun{lee2013} suchen, die Vorteile Regel- und Machine-Learning-basierter Systeme zu verheiraten und konnten so mit ihrem regel-basierten System das beste Ergebnis beim CoNLL-2011 \textit{shared task} erzielen. Im Rahmen der Sieb-Architektur werden nacheinander - absteigend nach ihrer Präzision geordnet - eine Reihe von deterministischen Koreferenzmodellen angewendet, wobei jedes Modell auf den Output seines Vorgängers aufbaut. Besonders das Entität-zentrische Modell, bei dem Merkmale über alle Vorkommen einer Entität geteilt werden, ermöglicht die globale Koreferenzresolution für regel-basierte Systeme.

\section{Allgemeine Implementation}

Die Klasse \texttt{SieveAnnotator} ruft für jedes Dokument die \texttt{decodeDocument}-Methode des \texttt{SieveDecoder} auf, der \texttt{corefResolver} als Interface implementiert. In dieser wird über alle \textit{mentions} dieses Dokumentes zehnmal iteriert, wobei bei jeder Iteration durch eine \texttt{SieveFactory} ein anderer \textit{sieve} aufgerufen wird. Hier filtern wir bereits alle indefiniten \textit{mentions} heraus, da diese sich meist auf generische Konzepte beziehen, die keinen Koreferenten besitzen.\\
Jeder \textit{sieve} erbt von der abstrakten \texttt{Sieve}-Klasse, die \texttt{runSieve}-Methode, die er für jede \textit{mention} aufruft und in der er unter den Antezedenzien nach einem Koreferenten für diese \textit{mention} sucht. \texttt{Sieve} enthält bereits alle \textit{utility}-Methoden, die von den Subklassen verwendet werden und von denen die meisten als Input eine \texttt{PairInstance} bestehend aus \textit{mention} und Antezedens annehmen. Zur Bestimmung mancher Merkmale verwenden wir darüber hinaus die in BART bereits implementierten \texttt{PairFeatureExtractors}.\\
Die Merkmale koreferenter \textit{mentions} werden in einer eigenen \texttt{DiscourseEntity}-Klasse geteilt. Weitere sprachspezifische Informationen ergänzten wir in den entsprechenden \texttt{LinguisticConstants} sowie durch Methoden zum Nachschlagen in zusammengestellten Listen im korrespondierenden \texttt{LanguagePlugin}.

\section{Details zu den einzelnen Sieves}

\begin{itemize}

\item \textbf{SpeakerIdentification} identifiziert den Sprecher und verbindet ihn mit möglichen koreferenten Pronomen in wörtlicher Rede. Da der von \citeasnoun{lee2013} verwendete OntoNotes-Korpus auch Telefon-Gespräche und Talk Shows umfasst, erzielt dieser Sieve bereits einen Recall von 8,7\% und einen F1-Score von 15,5\% (MUC). Da der TüBa-D/Z-Korpus hingegen nur aus Nachrichtenartikeln besteht, ist der Effekt dieses \textit{sieve} signifikant geringer. Handelt es sich bei einem der beiden \textit{mentions} um ein Vorfeld-Es, so wird keine Übereinstimmung gefunden. Numerus-Äquivalenz und eine Satzentfernung ${\leq}$ 1 müssen gegeben sein. Es wird mit BARTs \texttt{FE\_Speech} überprüft, ob sich eine der \textit{mentions} in wörtlicher Rede befindet und auf Grundlage der Position von ':' und '"' sowie Sprechverben ausgemacht, wo sich der Sprecher befindet, woraufhin schließlich sichergestellt wird, dass die sich in der wörtlichen Rede befindende \textit{mention} weder Reflexiv-, noch Relativ-, sondern lediglich Pronomen ist.

\item \textbf{StringMatch} sieht zwei \textit{mentions} als koreferent an, wenn sie exakt übereinstimmen (einschließlich Modifikatoren und Artikel). Wir überprüfen dies, indem wir den String der \textit{markables} der beiden \textit{mentions} vergleichen. Zudem schließen wir \textit{mentions} aus, deren String eine Datumsangabe enthält, da gleiche Daten in TüBa-D/Z nicht als koreferent gelten.\citeasnoun{lee2013} geben hier eine Präzision von über 90\% $B^3$ an, während wir für das Deutsche und den TüBa-D/Z-Korpus eine etwas geringere Genauigkeit von ca. 86\% erreichen.

\item \textbf{RelaxedStringMatch} gibt zwei \textit{mentions} als koreferent an, sofern sie identisch sind, wenn ihre Postmodifikatoren ignoriert werden.

\item \textbf{PreciseConstructs} matcht zwei \textit{mentions} als koreferent, wenn sie eine der folgenden Bedingungen erfüllen: 
\begin{itemize}
	\item Sie stehen gemeinsam in einer Appositions- oder Subjekt-Objekt-Konstruktion. Diese Bedingung ist für uns unerheblich, da diese Konstruktionen in TüBa-D/Z nicht annotiert werden.
	\item Die \textit{mention} modifiziert den Antezedens-Kopf. Hier weisen wir explizit den deutschen Relativpronomen ein Genus zu und überprüfen dessen Übereinstimmung zwischen \textit{mention} und Antezedens sowie Wortentfernung, wobei wir sicherstellen, dass das Antezedens nicht von einer weiteren \textit{mention} eingebettet wird, auf die sich das Relativpronomen eher beziehen könnte.
	\item Eine der \textit{mentions} ist ein Akronym der anderen. Hier berücksichtigen wir Eigenheiten des Deutschen, in dem auch Akronyme teilweise mit '-' getrennt werden.
	\item Eine der \textit{mentions} ist ein Demonym der anderen. Hierfür schlagen wir Demonyme in einer auf Wikipedia basierenden Liste nach, wobei dieses Kriterium im TüBa-D/Z-Korpus nicht von großer Bedeutung ist.
\end{itemize}
\citeasnoun{lee2013} geben für diesen \textit{sieve} eine $B^3$-Präzision von über 90\%, während wir eine Genauigkeit von ca. 81\% erhalten. Da das Relativpronomen-Kriterium in diesem \textit{sieve} überwiegt, liegt diese Differenz vermutlich in der hypotaktischen Struktur des Deutschen (im Vergleich zum Englischen), sowie in der domänspezifischen Häufigkeit extraponierter (und daher schwer zuordenbarer) Relativsätze begründet.

\item \textbf{StrictHeadMatchA} matcht zwei Mentions, wenn das Head Lemma der Anapher in den Heads der Antezedens Entity vorhanden ist und die folgenden Bedingungen erfüllt sind:

\begin{compactenum}[(i)]
 \item Die Modifikatoren der mention müssen unter den Modifikatoren des Antezedens sein.
 \item Alle Nicht-Stoppwörter der aktuellen Entität müssen in der Anzedens-Entität vorkommen.
 \item Keine der beiden NPs kann Kind des Konstituenten der anderen NP sein (\textit{i-within-i}).
\end{compactenum}

\item \textbf{StrictHeadMatchB} ignoriert Bedingung (i), während \textbf{StrictHeadMatchC} ebenfalls auf Bedingung (ii) verzichtet.

\item \textbf{ProperHeadNounMatch} weist zwei mentions als koreferent aus, wenn sie dasselbe \textit{head word} besitzen, sowie Bedingung (iii) erfüllen und (iv) keine unterschiedlichen Orte, Namen, Zahlen oder räumliche Modifikatoren aufweisen.

\item \textbf{RelaxedHeadMatch} matcht zwei mentions, wenn der Kopf der mention mit einem Wort in der Antezedens-Entität übereinstimmt, wobei beide \textit{named entities} desselben Typs sein müssen und Bedingungen (ii) und (iii) genügen müssen.

\item \textbf{PronounMatch} matcht Pronomen mit ihrem Antezedens, sofern diese morphologisch kompatibel sind und den Binding-Constraints genügen. Das Pronomen ist mit demjenigen Antezedens koreferent, das die höchste \textit{salience} \cite{wunsch2006} hat.

\end{itemize}

\section{Vergleich der Datenformate und Korpora}
Das XML-Format der TüBa-D/Z wird für BART in MMAX2 \cite{muller2006} konvertiert, das die Daten in verschiedenen \textit{markable levels} bereitstellt, wobei auf benötigte Diskurselemente, wie z.B. grammatische Funktionen mit \texttt{getDiscourseElementsByLevel} zugegriffen werden kann. Wie bereits eingangs erwähnt unterscheiden sich die Annotationsrichtlinien der verwendeten Korpora und ihre Datenformate hinsichtlich der Konstruktionen, die annotiert werden und ihrer Domäne.

\section{Evaluation}
Aufgrund der Schwierigkeit, Daten im MMAX2-Format zu analysieren und zu visualisieren, konvertierten wir die Texte in das HTML-Format, wobei wir koreferente mentions farbig markierten. Mithilfe der \texttt{Evaluation}-Klasse ließen wir uns ebenfalls die Details jedes matches und die Sieb-spezifische Performanz ausgeben.
\begin{table}[h]
\begin{tabular}{l||ll|l}
& \multicolumn{3}{c}{\textbf{MUC-Score}} \\ \hline
               & \textbf{Recall}		 & \textbf{Precision} & \textbf{F\_1}    \\ \hline
Unser System 	& 0.639      & 0.692              & 0.664  \\
BART  & 0.721 		 & 0.532     & 0.612
          
\end{tabular}
\caption{Vergleich mit BARTs Machine Learning-Konfiguration (XMLExperiment)}
\end{table}
\\

\begin{table}[h]
\begin{tabular}{l||ll|l}
& \multicolumn{3}{c}{\textbf{MUC-Score}} \\ \hline
	                 & \textbf{Recall} & \textbf{Precision} & \textbf{F\_1} \\ \hline
SpeakerIdentification & 0.004  & 0.637     & 0.008    \\ 
+StringMatch          & 0.157  & 0.857     & 0.265    \\ 
+RelaxedStringMatch   & 0.180  & 0.825     & 0.295    \\ 
+PreciseConstructs    & 0.237  & 0.822     & 0.367    \\ 
+HeadMatchA           & 0.291  & 0.809     & 0.427    \\ 
+HeadMatchB           & 0.351  & 0.775     & 0.483    \\ 
+HeadMatchC           & 0.353  & 0.770     & 0.484    \\ 
+ProperHeadNounMatch  & 0.354  & 0.771     & 0.485    \\ 
+RelaxedHeadMatch     & 0.379  & 0.771     & 0.508    \\ 
+PronounMatch         & 0.640  & 0.691     & 0.664    \\ 

\end{tabular}
\caption{Performanz der einzelnen Siebe}
\end{table}
Als Testkorpus verwendeten wir die ersten 99 Dokumente der TüBa-D/Z, wobei wir BARTs Machine Learning-Komponente auf den restlichen Dokumenten der TüBa-D/Z (Nr. 100 - Nr. 3528) trainierten.\\
2. Tabelle: Vergleich mit Stanford-System mit CoNLL Scorer auf OntoNotes\\


\clearpage

\nocite{*}
\renewcommand*{\refname}{} % This will define heading of bibliography to be empty, so you can...
\section{Literatur}  
\bibliography{lit}{}
\bibliographystyle{agsm}

\end{document}
