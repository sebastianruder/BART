\documentclass{scrartcl}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
%für Windows
\usepackage[utf8]{inputenc}

% for German citation style
% \usepackage[options]{natbib}
\usepackage{harvard} 
\renewcommand{\harvardand}{und} 

%\setkomafont{disposition}{\normalfont\bfseries}
\author{Julian Baumann, Xenia K\"uhling, Sebastian Ruder}
\date{08. August 2014}
\title{Regelbasierte Koreferenz mit BART}
\subtitle{Algorithmen und Implementation zum Softwareprojekt im SS14}

%Brücke zwischen algorithmischer und implementatorischer Ebene
%abstrakter, mit Bezug zu den computerlinguistischen Fragestellungen

\begin{document}
\maketitle

\section{BART im Vergleich zu Stanford}

BART unternimmt automatische Koreferenzresolution mithilfe einer modularen Pipeline, die aus einer Vorverarbeitungsphase (Daten von MMAX2-Annotationsebenen werden aggregiert), der Extraktion der NP-Kandidaten, der Extraktion der NP-Merkmale und der Kandidatenpaare sowie aus einem Resolutionsmodell besteht.
 BART verwendet momentan einen auf einem Ansatz von Soon basierenden Resolutionsalgorithmus, der Kandidaten-NPs hinsichtlich ihrer Merkmale paarweise vergleicht. Statt diesem soll nun das Resolutionssystem der Stanford-NLP-Gruppe (im Folgenden Stanford-System) implementiert werden, das sich durch seine Sieb-Architektur auszeichnet. Obwohl es haupts\"achlich auf Regeln basiert, konnte es dennoch das beste Ergebnis beim CoNLL-2011 shared task erzielen. Im Rahmen der Sieb-Architektur werden nacheinander - absteigend nach ihrer Pr\"azision geordnet - eine Reihe von deterministischen Koreferenzmodellen angewendet, wobei jedes Modell auf den Output seines Vorg\"angers aufbaut. Besonders das Entit\"at-zentrische Modell, in bei dem Merkmale \"uber alle Vorkommen einer Entit\"at geteilt werden, bietet einen deutlichen Wissensgewinn, der von Nutzen f\"ur BARTs Performanz sein wird.
Vergleich Regel-basiert mit Machine-learning

\section{Allgemeine Implementation}

SieveFactory, abstrakte Klassen, von der die einzelnen Sieves erben

\section{Details zu den einzelnen Sieves}

\begin{itemize}
\item \textbf{Speaker Identification:} Es werden Sprecher identifiziert und mit m\"oglichen koreferenten Pronomen verbunden. Da der von der Stanford-Gruppe verwendete OntoNotes-Korpus auch Telefon-Gespräche und Talk Shows umfasst, erzielt dieser Sieve bereits einen Recall von 8,7\% und einen F1-Score von 15,5\% (MUC-Score); da der TüBa-D/Z-Korpus hingegen nur aus Nachrichtenartikeln besteht, ist der Effekt dieses Sieves signifikant geringer. Da es sich vor allem um indirekte Rede handelt, schlagen wir Verben nach, die synonym mit "sagen" sind.

\item \textbf{String Match:} Zwei mentions werden als koreferent angesehen, wenn sie exakt übereinstimmen (einschließlich Modifikatoren und Artikel). \citeasnoun{lee2013} geben hier eine Precision von über 90\% $B^3$ an. Für das Deutsche und den TüBa-D/Z-Korpus erreicht dieser Sieve allerdings nur eine Precision von ??.
\item \textbf{Relaxed String Match:} Zwei nominale Entit\"aten sind koreferent, wenn ihre K\"opfe gleich sind.
\item \textbf{Precise Constructs:} Zwei Entit\"aten sind koreferent, wenn sie gemeinsam in einer Appositions- oder Subjekt-Objekt-Konstruktion stehen. Wenn die Entit\"at ein zum Kopf des Antezedens geh\"origes Relativpronomen ist, ein Akronym oder ein Demonym ist. 
\item \textbf{Strict Head Match A, Strict Head Match B, Strict Head Match C, Proper Head Noun Match, Relaxed Head Match:} Diese Regeln bezeichnen Entit\"aten als koreferent, wenn sie denselben Kopf haben und bestimmte Bedingungen erf\"ullen. 
\item \textbf{Pronoun Match:} Pronominale Koreferenz besteht, wenn bestimmte Agreement-Bedingungen erf\"ullt sind. Z.B.: Numerus, Genus, Person, Belebtheit, Satzentfernung zwischen Pronomen und Antezedens ${\leq}$ 3
\end{itemize}

Vorerst sollen String Match und Pronoun Match implementiert werden. (?!)


\section{Vergleich der Datenformate}
Eigenheiten/Besonderheiten MMAX2
Das XML-Format der TüBa-D/Z wird für BART in MMAX2 konvertiert (zitieren: Müller and Strube, 2006 -Multi-level annotation of linguistic data with MMAX2).
Hierbei gehen durch die Verwendung von MiniDiscourse in BART bspw. grammatische Funktionen verloren.

Vergleich mit OntoNotes-Format

\section{Evaluation}
Evaluationstabellen
1. Tabelle: Vergleich mit BART Machine Learning-Konfiguration XMLExperiment auf TüBa-D/Z
2. Tabelle: Vergleich mit Stanford-System mit CoNLL Scorer auf OntoNotes


\nocite{*}
\renewcommand*{\refname}{} % This will define heading of bibliography to be empty, so you can...
\section{Literatur}  
\bibliography{lit}
\bibliographystyle{abbrv}

\end{document}
